todo:
learner
    saving & loading: model & optimizer state
    checkpointing

findings:
validation sequence length shouldnt exceed training sequence length. if unavoidable, use prepadding to train on the same length. through prepadding the model gets conditioned to propagate through the whole length. dont know why that would help, since just zeros, but loss is somehow better. i think only needed if i use the whole history length, otherwise no padding and same val length.